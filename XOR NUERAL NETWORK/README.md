# XOR Neural Network

This program trains a **small neural network** to solve the classic XOR problem.  
It learns to classify the inputs `(0,0)`, `(0,1)`, `(1,0)`, `(1,1)` correctly using **forward and backward propagation**.

## What it does
- Implements a **2-layer neural network** from scratch using NumPy  
- Uses **sigmoid activation functions** for hidden and output layers  
- Trains the network using **gradient descent**  
- Shows **final predictions** for XOR inputs  
- Plots **loss over time** to visualize learning  
- Plots **decision boundary** to show how the network separates classes

## Real-life examples
1. Learning **non-linear patterns** in data (e.g., logic circuits or XOR-like logic)  
2. Binary classification tasks where simple linear models fail  
3. Foundation for understanding how **neural networks solve complex problems**

## What I learned
- How a neural network can **learn non-linear patterns**  
- How **forward pass, backward pass, and gradient descent** work together  
- How **activations and derivatives** influence predictions  
- How to **visualize learning and decision boundaries** for clarity
